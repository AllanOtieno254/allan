# ğŸŒ Designing Responsible and Fair AI Systems âš–ï¸

## ğŸ“˜ Project Overview

This repository contains a full academic and technical submission for the PLP Academy assignment: **"Designing Responsible and Fair AI Systems."**  
The project is designed to address the ethical challenges of artificial intelligence (AI) through theory, case study analysis, and practical auditing of AI systems using fairness metrics and bias mitigation techniques.

We focus on creating AI systems that are **transparent, fair, inclusive, and trustworthy**, as guided by the **EU Ethics Guidelines for Trustworthy AI** and core ethical principles like **justice, autonomy, and non-maleficence**.

---

## ğŸ¯ Objectives

- Develop a deep understanding of **AI ethics principles**.
- Analyze real-world **ethical dilemmas** in deployed AI systems.
- Perform a **bias audit** on the COMPAS dataset using IBM's AI Fairness 360 toolkit.
- Reflect on personal AI projects and how to ethically improve them.
- Propose responsible AI use policies in healthcare (Bonus Task).

---

## ğŸ§± Project Structure


---

## ğŸ§ª Part 3: COMPAS Bias Audit (Technical Component)

### âœ… Dataset:
- **COMPAS (Correctional Offender Management Profiling for Alternative Sanctions)**
- Provided by ProPublica, it predicts recidivism risks but has been shown to exhibit racial bias.

### ğŸ” Tasks Completed:
- Analyzed racial disparities in **false positive rates** and **statistical parity**.
- Measured **disparate impact ratio**.
- Applied mitigation algorithms such as **Reweighing** and **Equalized Odds Postprocessing**.
- Visualized group-wise performance and bias metrics.

### ğŸ“Š Tools & Libraries Used:
- Python 3.x
- Pandas
- Matplotlib & Seaborn
- IBM AIF360 Toolkit (https://github.com/IBM/AIF360)
- Jupyter Notebook

---

## âš–ï¸ Part 2: Case Study Analysis

### Case 1: Amazon's Biased Hiring Tool
- Identified gender bias due to historically male-dominated training data.
- Proposed data balancing, gender-neutral feature selection, and fairness-aware modeling.

### Case 2: Facial Recognition in Policing
- Highlighted risks such as false arrests, over-surveillance, and disproportionate targeting.
- Proposed strict auditing, accuracy thresholds, and banning in real-time law enforcement use.

---

## ğŸ§  Part 1: Theoretical Understanding

Answered key questions covering:
- Definitions and real-world examples of **algorithmic bias**.
- Importance of **transparency** and **explainability**.
- The role of **GDPR** in shaping fair and lawful AI development.

Also matched ethical principles (Justice, Non-maleficence, Autonomy, Sustainability) to their definitions.

---

## ğŸ’¡ Part 4: Ethical Reflection

A personal reflection on how the lessons from this project will influence future AI system design, ensuring:
- Bias audits are part of every pipeline.
- Data is inclusive and diverse.
- Decisions are explainable to all users.

---

## ğŸ Bonus Task: Ethical AI in Healthcare

A one-page policy document proposing:
- Informed consent protocols.
- Bias mitigation strategies in medical AI models.
- Transparency and accountability mechanisms in deployment.

---

## ğŸš€ Getting Started

### 1. Clone the Repository
```bash
git clone https://github.com/your-username/designing-fair-ai-systems.git
cd designing-fair-ai-systems
